{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNG562-Assignment1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisanuro/CNG562-Assignment-1/blob/master/CNG562_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEX_vKmhbU97",
        "colab_type": "text"
      },
      "source": [
        "# **CNG 562 - Assignment #1**\n",
        "\n",
        "Linear Regression vs Logistic Regression using Iris Dataset\\\n",
        "Comparing:\n",
        "*   Random 1-Hold Out\n",
        "*   5-Fold\n",
        "*   10-Fold\n",
        "*   Strafied 1-Hold Out\n",
        "\n",
        "\\\n",
        "Nisa Nur Odabaş\\\n",
        "Kaan Taha Köken\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7xb3nOt1MJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt  \n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn import metrics, datasets, preprocessing\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSeSvKNyd4WU",
        "colab_type": "text"
      },
      "source": [
        "**K-Fold method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNATKa--c7Z0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def kFold(foldNumber, X_train, Y_train):\n",
        "\n",
        "  kf = KFold(n_splits=foldNumber, shuffle=False)  \n",
        "\n",
        "  logReg = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "  linReg = LinearRegression()  \n",
        "\n",
        "  cv_result_log = cross_val_score(logReg, X_train, Y_train, cv=kf, scoring='accuracy')\n",
        "  cv_result_lin = cross_val_score(linReg, X_train, Y_train, cv=kf, scoring='neg_mean_squared_error')\n",
        "\n",
        "  print(str(foldNumber) + \"Fold\")\n",
        "  print(\"Logistic Regression Accuracy: \", cv_result_log.mean())\n",
        "  print(\"Linear Regression Accuracy: \", 1 + cv_result_lin.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyf75fcUeBB5",
        "colab_type": "text"
      },
      "source": [
        "**Random 1-Hold Out method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D9wHIq3ceJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def randomOneHoldout(X_train, Y_train):\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=1)\n",
        "  \n",
        "  logReg = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "  linReg = LinearRegression()\n",
        "\n",
        "  logReg.fit(x_train, y_train)\n",
        "  linReg.fit(x_train, y_train)\n",
        "\n",
        "  y_pred_log = logReg.predict(x_test)\n",
        "  y_pred_lin = linReg.predict(x_test)\n",
        "  \n",
        "  print(\"Random One Hold Out\")\n",
        "  print(\"Logistic Regression Accuracy: \", 1 - metrics.mean_squared_error(y_test, y_pred_log))\n",
        "  print(\"Linear Regression Accuracy: \", 1 - metrics.mean_squared_error(y_test, y_pred_lin))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPWS39aXeI6n",
        "colab_type": "text"
      },
      "source": [
        "**Stratified 1-Hold Out method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evz_cnDudAiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stratifiedOneHoldout(X_train, Y_train):\n",
        "  \n",
        "  x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=1, stratify=Y_train)\n",
        "  \n",
        "  logReg = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "  linReg = LinearRegression()\n",
        "\n",
        "  logReg.fit(x_train, y_train)\n",
        "  linReg.fit(x_train, y_train)\n",
        "\n",
        "  y_pred_log = logReg.predict(x_test)\n",
        "  y_pred_lin = linReg.predict(x_test)\n",
        "  \n",
        "  print(\"Stratified\")\n",
        "  print(\"Logistic Regression Accuracy: \", 1 - metrics.mean_squared_error(y_test, y_pred_log))\n",
        "  print(\"Linear Regression Accuracy: \", 1 - metrics.mean_squared_error(y_test, y_pred_lin))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_2B_gqEeOd9",
        "colab_type": "text"
      },
      "source": [
        "**Displaying accuracies for all validation methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5LKjla4N3Al",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def displayAccuracy(X, Y):\n",
        "    \n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
        "    \n",
        "    kFold(5, X_train, Y_train)    \n",
        "    kFold(10, X_train, Y_train)    \n",
        "    randomOneHoldout(X_train, Y_train)\n",
        "    stratifiedOneHoldout(X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fXFXbbvecNi",
        "colab_type": "text"
      },
      "source": [
        "**Round method for linear regression prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxbKsVBDL_ru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def roundPredict(p):\n",
        "    r = p.copy()\n",
        "    for i in range(len(r)):\n",
        "        if r[i] <= 0.5: r[i] = 0\n",
        "        elif r[i] >= 1.5: r[i] = 2\n",
        "        else: r[i] = 1\n",
        "    return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-5fN1oyejo-",
        "colab_type": "text"
      },
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs4klAsSguXu",
        "colab_type": "code",
        "outputId": "d3ddb266-11cd-4c6e-a0b3-a2fab9cc20c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  iris = datasets.load_iris()\n",
        "  \n",
        "  X = iris.data\n",
        "  Y = iris.target\n",
        "  \n",
        "  # L1 normalization\n",
        "  l1_norm = preprocessing.normalize(X, norm=\"l1\")\n",
        "  # Mean removal\n",
        "  mean_removal = preprocessing.scale(X)\n",
        "\n",
        "  '''#mean & standart deviation before mean removal \n",
        "  print(X.mean(axis=0))\n",
        "  print(X.std(axis=0))\n",
        "\n",
        "  #mean & standart deviation after mean removal \n",
        "  print(mean_removal.mean(axis=0))\n",
        "  print(mean_removal.std(axis=0))'''\n",
        "\n",
        "  #Displaying result according to each type of methods and regression model\n",
        "  print(\"\\nRaw: \")\n",
        "  displayAccuracy(X,Y)\n",
        "  print(\"\\nL1 Normalization: \")\n",
        "  displayAccuracy(l1_norm,Y)\n",
        "  print(\"\\nMean Removal: \")\n",
        "  displayAccuracy(mean_removal,Y)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Raw: \n",
            "5Fold\n",
            "Logistic Regression Accuracy:  0.9416666666666668\n",
            "Linear Regression Accuracy:  0.9505573483173648\n",
            "10Fold\n",
            "Logistic Regression Accuracy:  0.9333333333333332\n",
            "Linear Regression Accuracy:  0.9504757453551504\n",
            "Random One Hold Out\n",
            "Logistic Regression Accuracy:  0.9583333333333334\n",
            "Linear Regression Accuracy:  0.9596940114241354\n",
            "Stratified\n",
            "Logistic Regression Accuracy:  0.9583333333333334\n",
            "Linear Regression Accuracy:  0.9613449341091412\n",
            "\n",
            "L1 Normalization: \n",
            "5Fold\n",
            "Logistic Regression Accuracy:  0.6916666666666667\n",
            "Linear Regression Accuracy:  0.9249382972717285\n",
            "10Fold\n",
            "Logistic Regression Accuracy:  0.6916666666666667\n",
            "Linear Regression Accuracy:  0.9228036562601725\n",
            "Random One Hold Out\n",
            "Logistic Regression Accuracy:  0.75\n",
            "Linear Regression Accuracy:  0.9311930338541666\n",
            "Stratified\n",
            "Logistic Regression Accuracy:  0.7083333333333333\n",
            "Linear Regression Accuracy:  0.9382756551106771\n",
            "\n",
            "Mean Removal: \n",
            "5Fold\n",
            "Logistic Regression Accuracy:  0.8916666666666668\n",
            "Linear Regression Accuracy:  0.9505573483173648\n",
            "10Fold\n",
            "Logistic Regression Accuracy:  0.8833333333333334\n",
            "Linear Regression Accuracy:  0.9504757453551504\n",
            "Random One Hold Out\n",
            "Logistic Regression Accuracy:  1.0\n",
            "Linear Regression Accuracy:  0.9596940114241352\n",
            "Stratified\n",
            "Logistic Regression Accuracy:  0.9166666666666666\n",
            "Linear Regression Accuracy:  0.9613449341091413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwS89vg5fEhF",
        "colab_type": "text"
      },
      "source": [
        "# **Final**\n",
        "**Training and Testing using:**\n",
        "* **Raw data**\n",
        "* **Stratified 1-Hold Out**\n",
        "* **Linear Regression**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H8jMNi4g1FU",
        "colab_type": "text"
      },
      "source": [
        "**Dividing Train and Test sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCRTm6pZg8nN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1, stratify=Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgm3mSbMhJy2",
        "colab_type": "text"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oow4dOrkUk0y",
        "colab_type": "code",
        "outputId": "4d4ef968-5a24-416e-8312-b742373023e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "  x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=1, stratify=Y_train)\n",
        " \n",
        "  linReg = LinearRegression()\n",
        "  linReg.fit(x_train, y_train)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBrwECDVhi94",
        "colab_type": "text"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBGpYYmvdDRt",
        "colab_type": "text"
      },
      "source": [
        "**Finding Errors for Model**\\\n",
        "Splitting dataset into 4.\n",
        "*   Train\n",
        "*   Train Dev\n",
        "*   Dev\n",
        "*   Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP1bK9dKPfHT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "dcb87bc8-f703-4c89-ddb4-3f161841dc57"
      },
      "source": [
        "  trainDev_pred = linReg.predict(x_test)\n",
        "  rounded_lin = roundPredict(trainDev_pred)\n",
        "  \n",
        "  print(\"Train-Train Dev,   e1:\", metrics.mean_squared_error(y_test, trainDev_pred),\"\\n\")\n",
        "  print(\"Rounded Stratify One Hold Out - Only train set\")\n",
        "  print(\"Linear Regression Accuracy: \", 1 - metrics.mean_squared_error(y_test, trainDev_pred))\n",
        "  print(\"Linear Regression R^2 score: \", metrics.r2_score(y_test, trainDev_pred))\n",
        "\n",
        "  print(\"\\nY_pred_lin     \\t       Y_test\\trounded\")\n",
        "  for i, (j, k) in sorted(zip(trainDev_pred, zip(y_test, rounded_lin))):\n",
        "    print(i , \"\\t\" , j, \"\\t\", k)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train-Train Dev,   e1: 0.0789685015383695 \n",
            "\n",
            "Rounded Stratify One Hold Out - Only train set\n",
            "Linear Regression Accuracy:  0.9210314984616305\n",
            "Linear Regression R^2 score:  0.8815472476924457\n",
            "\n",
            "Y_pred_lin     \t       Y_test\trounded\n",
            "-0.13338533425244264 \t 0 \t 0.0\n",
            "-0.1083659071882706 \t 0 \t 0.0\n",
            "-0.06122542437237788 \t 0 \t 0.0\n",
            "-0.05329118074897954 \t 0 \t 0.0\n",
            "-0.0429126159997677 \t 0 \t 0.0\n",
            "-0.0242791109138423 \t 0 \t 0.0\n",
            "0.0009096628014323427 \t 0 \t 0.0\n",
            "0.8235121047102323 \t 1 \t 1.0\n",
            "0.8780486937147116 \t 1 \t 1.0\n",
            "1.161330224826963 \t 1 \t 1.0\n",
            "1.1870391393881594 \t 1 \t 1.0\n",
            "1.2992859080092798 \t 1 \t 1.0\n",
            "1.3299274028519623 \t 1 \t 1.0\n",
            "1.3879556985627717 \t 1 \t 1.0\n",
            "1.3950051665970737 \t 2 \t 1.0\n",
            "1.416967768397909 \t 2 \t 1.0\n",
            "1.5535486326267118 \t 2 \t 2.0\n",
            "1.6638520161866535 \t 2 \t 2.0\n",
            "1.6638520161866535 \t 2 \t 2.0\n",
            "1.8198963303604019 \t 2 \t 2.0\n",
            "2.0112270682746454 \t 2 \t 2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-4pdzAtLzWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dev_x, Test_x, Dev_y, Test_y = train_test_split(X_test, Y_test, test_size=0.5, random_state=1, stratify=Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6tMJf3yRw5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "5e18d68f-dede-41c3-e027-af4c7db31729"
      },
      "source": [
        "dev_pred = linReg.predict(Dev_x)\n",
        "rounded_lin = roundPredict(dev_pred)\n",
        "\n",
        "print(\"Train-Dev,   e2\", metrics.mean_squared_error(Dev_y, dev_pred),\"\\n\")\n",
        "print(\"Rounded Stratify One Hold Out - Test set\")\n",
        "print(\"Linear Regression Accuracy: \", 1 - metrics.mean_squared_error(Dev_y, dev_pred))\n",
        "print(\"Linear Regression R^2 score: \", metrics.r2_score(Dev_y, dev_pred))\n",
        "\n",
        "print(\"\\nY_pred_lin     \\t      Y_test\\trounded\")\n",
        "for i, (j, k) in sorted(zip(dev_pred, zip(Dev_y, rounded_lin))):\n",
        "    print(i , \"\\t\" , j, \"\\t\", k)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train-Dev,   e2 0.04154402265621487 \n",
            "\n",
            "Rounded Stratify One Hold Out - Test set\n",
            "Linear Regression Accuracy:  0.9584559773437852\n",
            "Linear Regression R^2 score:  0.938883565454079\n",
            "\n",
            "Y_pred_lin     \t      Y_test\trounded\n",
            "-0.1451056726255265 \t 0 \t 0.0\n",
            "-0.10091460304061714 \t 0 \t 0.0\n",
            "-0.04522063580412056 \t 0 \t 0.0\n",
            "-0.03300609802608376 \t 0 \t 0.0\n",
            "0.0022514364253043984 \t 0 \t 0.0\n",
            "0.03221849251171449 \t 0 \t 0.0\n",
            "0.03867050514924514 \t 0 \t 0.0\n",
            "0.23727904451950455 \t 0 \t 0.0\n",
            "0.8134542366743358 \t 1 \t 1.0\n",
            "1.107129381276551 \t 1 \t 1.0\n",
            "1.1287122995934598 \t 1 \t 1.0\n",
            "1.14436334611876 \t 1 \t 1.0\n",
            "1.2233068580498874 \t 1 \t 1.0\n",
            "1.2550989944649051 \t 1 \t 1.0\n",
            "1.369974479603694 \t 1 \t 1.0\n",
            "1.5527626257975506 \t 2 \t 2.0\n",
            "1.6955036952396758 \t 2 \t 2.0\n",
            "1.7030733041199 \t 2 \t 2.0\n",
            "1.7195392467634683 \t 2 \t 2.0\n",
            "1.8532104797506117 \t 2 \t 2.0\n",
            "1.9358037851099639 \t 2 \t 2.0\n",
            "2.0041734441998083 \t 2 \t 2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUhWtFNvBupT",
        "colab_type": "code",
        "outputId": "5b9c5c57-1b37-4a0f-d0c8-0d29dc2f1b89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "test_pred = linReg.predict(Test_x)\n",
        "rounded_lin = roundPredict(test_pred)\n",
        "\n",
        "\n",
        "print(\"Train-Test,   e3: \", metrics.mean_squared_error(Test_y, test_pred),\"\\n\")\n",
        "print(\"Rounded Stratify One Hold Out - Test set\")\n",
        "print(\"Linear Regression Accuracy: \", 1 - metrics.mean_squared_error(Test_y, test_pred))\n",
        "print(\"Linear Regression R^2 score: \", metrics.r2_score(Test_y, test_pred))\n",
        "\n",
        "print(\"\\nY_pred_lin     \\t      Y_test\\trounded\")\n",
        "for i, (j, k) in sorted(zip(test_pred, zip(Test_y, rounded_lin))):\n",
        "  print(i , \"\\t\" , j, \"\\t\", k)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train-Test,   e3:  0.05381451146066997 \n",
            "\n",
            "Rounded Stratify One Hold Out - Test set\n",
            "Linear Regression Accuracy:  0.94618548853933\n",
            "Linear Regression R^2 score:  0.9172445448758884\n",
            "\n",
            "Y_pred_lin     \t      Y_test\trounded\n",
            "-0.09366984691424335 \t 0 \t 0.0\n",
            "-0.06334904878487607 \t 0 \t 0.0\n",
            "-0.051639603112009214 \t 0 \t 0.0\n",
            "-0.037150090859261575 \t 0 \t 0.0\n",
            "-0.029215847235863235 \t 0 \t 0.0\n",
            "-0.001534658324381044 \t 0 \t 0.0\n",
            "0.0894168433635038 \t 0 \t 0.0\n",
            "0.841651410391205 \t 1 \t 1.0\n",
            "0.9431482428602571 \t 1 \t 1.0\n",
            "1.1320305036081524 \t 1 \t 1.0\n",
            "1.2178650296226965 \t 1 \t 1.0\n",
            "1.2823039805603245 \t 1 \t 1.0\n",
            "1.2941689323359389 \t 1 \t 1.0\n",
            "1.3312624398161523 \t 1 \t 1.0\n",
            "1.3581508852387913 \t 1 \t 1.0\n",
            "1.4436785550883753 \t 2 \t 1.0\n",
            "1.573519755295974 \t 2 \t 2.0\n",
            "1.7337478442922245 \t 2 \t 2.0\n",
            "1.738835930676458 \t 2 \t 2.0\n",
            "1.7658208954312336 \t 2 \t 2.0\n",
            "1.9108808773779289 \t 2 \t 2.0\n",
            "1.9344578554455574 \t 2 \t 2.0\n",
            "2.130589028360361 \t 2 \t 2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43p8lSuCPCJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "0f5a8728-e726-4edb-e23e-c2227224e49a"
      },
      "source": [
        "devTest_pred = linReg.predict(X_test)\n",
        "rounded_lin = roundPredict(devTest_pred)\n",
        "\n",
        "\n",
        "print(\"Train-(Dev+Test),   e4: \", metrics.mean_squared_error(Y_test, devTest_pred),\"\\n\")\n",
        "print(\"Rounded Stratify One Hold Out - Test set\")\n",
        "print(\"Linear Regression Accuracy: \", 1 - metrics.mean_squared_error(Y_test, devTest_pred))\n",
        "print(\"Linear Regression R^2 score: \", metrics.r2_score(Y_test, devTest_pred))\n",
        "\n",
        "print(\"\\nY_pred_lin     \\t      Y_test\\trounded\")\n",
        "for i, (j, k) in sorted(zip(devTest_pred, zip(Y_test, rounded_lin))):\n",
        "  print(i , \"\\t\" , j, \"\\t\", k)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train-(Dev+Test),   e4:  0.04781560582293637 \n",
            "\n",
            "Rounded Stratify One Hold Out - Test set\n",
            "Linear Regression Accuracy:  0.9521843941770636\n",
            "Linear Regression R^2 score:  0.9282765912655955\n",
            "\n",
            "Y_pred_lin     \t      Y_test\trounded\n",
            "-0.1451056726255265 \t 0 \t 0.0\n",
            "-0.10091460304061714 \t 0 \t 0.0\n",
            "-0.09366984691424335 \t 0 \t 0.0\n",
            "-0.06334904878487607 \t 0 \t 0.0\n",
            "-0.051639603112009214 \t 0 \t 0.0\n",
            "-0.04522063580412056 \t 0 \t 0.0\n",
            "-0.037150090859261575 \t 0 \t 0.0\n",
            "-0.03300609802608376 \t 0 \t 0.0\n",
            "-0.029215847235863235 \t 0 \t 0.0\n",
            "-0.001534658324381044 \t 0 \t 0.0\n",
            "0.0022514364253043984 \t 0 \t 0.0\n",
            "0.03221849251171449 \t 0 \t 0.0\n",
            "0.03867050514924514 \t 0 \t 0.0\n",
            "0.0894168433635038 \t 0 \t 0.0\n",
            "0.23727904451950455 \t 0 \t 0.0\n",
            "0.8134542366743358 \t 1 \t 1.0\n",
            "0.841651410391205 \t 1 \t 1.0\n",
            "0.9431482428602571 \t 1 \t 1.0\n",
            "1.107129381276551 \t 1 \t 1.0\n",
            "1.1287122995934598 \t 1 \t 1.0\n",
            "1.1320305036081524 \t 1 \t 1.0\n",
            "1.14436334611876 \t 1 \t 1.0\n",
            "1.2178650296226965 \t 1 \t 1.0\n",
            "1.2233068580498874 \t 1 \t 1.0\n",
            "1.2550989944649051 \t 1 \t 1.0\n",
            "1.2823039805603245 \t 1 \t 1.0\n",
            "1.2941689323359389 \t 1 \t 1.0\n",
            "1.3312624398161523 \t 1 \t 1.0\n",
            "1.3581508852387913 \t 1 \t 1.0\n",
            "1.369974479603694 \t 1 \t 1.0\n",
            "1.4436785550883753 \t 2 \t 1.0\n",
            "1.5527626257975506 \t 2 \t 2.0\n",
            "1.573519755295974 \t 2 \t 2.0\n",
            "1.6955036952396758 \t 2 \t 2.0\n",
            "1.7030733041199 \t 2 \t 2.0\n",
            "1.7195392467634683 \t 2 \t 2.0\n",
            "1.7337478442922245 \t 2 \t 2.0\n",
            "1.738835930676458 \t 2 \t 2.0\n",
            "1.7658208954312336 \t 2 \t 2.0\n",
            "1.8532104797506117 \t 2 \t 2.0\n",
            "1.9108808773779289 \t 2 \t 2.0\n",
            "1.9344578554455574 \t 2 \t 2.0\n",
            "1.9358037851099639 \t 2 \t 2.0\n",
            "2.0041734441998083 \t 2 \t 2.0\n",
            "2.130589028360361 \t 2 \t 2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjUToaP_i4wD",
        "colab_type": "text"
      },
      "source": [
        "**Predicting [6, 3, 5, 1.5]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czCbG4-KR6ZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred = linReg.predict([[6, 3, 5, 1.5]])\n",
        "rounded = roundPredict(Y_pred.copy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s_nKHMuSJqv",
        "colab_type": "code",
        "outputId": "8734f40a-a7bc-4a65-912d-4f3709863ef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"Prediction: \\t\\t\",Y_pred)\n",
        "print(\"Predicted class: \\t\", rounded)\n",
        "print(\"Mean squared error: \\t\", metrics.mean_squared_error(rounded, Y_pred))\n",
        "print(\"Mean absolute error: \\t\", metrics.mean_absolute_error(rounded, Y_pred))"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: \t\t [1.35496898]\n",
            "Predicted class: \t [1.]\n",
            "Mean squared error: \t 0.1260029785694296\n",
            "Mean absolute error: \t 0.3549689825455593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Em11tVCvzAd",
        "colab_type": "text"
      },
      "source": [
        "**ROC**\\\n",
        "Using rounded predictions since linear regression has no ROC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4B7Xf47fd9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "8ee554cd-48f0-46ff-cef9-262383694194"
      },
      "source": [
        "scores=[]\n",
        "\n",
        "for i, j in sorted(zip(devTest_pred, Y_test)):\n",
        "    scores.append(float(j)- i)\n",
        "scores"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1451056726255265,\n",
              " 0.10091460304061714,\n",
              " 0.09366984691424335,\n",
              " 0.06334904878487607,\n",
              " 0.051639603112009214,\n",
              " 0.04522063580412056,\n",
              " 0.037150090859261575,\n",
              " 0.03300609802608376,\n",
              " 0.029215847235863235,\n",
              " 0.001534658324381044,\n",
              " -0.0022514364253043984,\n",
              " -0.03221849251171449,\n",
              " -0.03867050514924514,\n",
              " -0.0894168433635038,\n",
              " -0.23727904451950455,\n",
              " 0.1865457633256642,\n",
              " 0.15834858960879505,\n",
              " 0.05685175713974289,\n",
              " -0.10712938127655103,\n",
              " -0.12871229959345976,\n",
              " -0.13203050360815238,\n",
              " -0.1443633461187599,\n",
              " -0.2178650296226965,\n",
              " -0.22330685804988737,\n",
              " -0.25509899446490514,\n",
              " -0.28230398056032446,\n",
              " -0.2941689323359389,\n",
              " -0.33126243981615233,\n",
              " -0.35815088523879135,\n",
              " -0.3699744796036939,\n",
              " 0.5563214449116247,\n",
              " 0.44723737420244936,\n",
              " 0.4264802447040259,\n",
              " 0.3044963047603242,\n",
              " 0.2969266958801,\n",
              " 0.28046075323653175,\n",
              " 0.2662521557077755,\n",
              " 0.26116406932354197,\n",
              " 0.23417910456876645,\n",
              " 0.1467895202493883,\n",
              " 0.08911912262207111,\n",
              " 0.06554214455444263,\n",
              " 0.06419621489003613,\n",
              " -0.004173444199808252,\n",
              " -0.1305890283603608]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61U-P59TWpq2",
        "colab_type": "code",
        "outputId": "67875490-0b08-4ad7-8d53-3aa3e19ccaae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "#Prediction of the model\n",
        "temp = Y_test.copy()\n",
        "for i in range(len(Y_test)):\n",
        "    if Y_test[i] != 0: temp[i] = -1\n",
        "    else: temp[i] = 1\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(temp, scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print(fpr)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.13333333 0.13333333 0.26666667\n",
            " 0.26666667 0.6        0.6        0.63333333 0.63333333 0.73333333\n",
            " 0.73333333 0.76666667 0.76666667 0.83333333 0.83333333 0.96666667\n",
            " 0.96666667 1.        ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxpgU/R97Xl3wYov6UwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv3gtMxbeBoJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86eaeab4-e80a-41fc-ee9b-51231fa1ffa8"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from scipy import interp\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "# Import some data to play with\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Binarize the output\n",
        "y = label_binarize(y, classes=[0, 1, 2])\n",
        "n_classes = y.shape[1]\n",
        "\n",
        "# Add noisy features to make the problem harder\n",
        "random_state = np.random.RandomState(0)\n",
        "n_samples, n_features = X.shape\n",
        "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
        "\n",
        "# shuffle and split training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n",
        "                                                    random_state=0)\n",
        "\n",
        "# Learn to predict each class against the other\n",
        "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
        "                                 random_state=random_state))\n",
        "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
        "print(y_score, y_test)\n",
        "# Compute ROC curve and ROC area for each class\n",
        "#fpr = dict()\n",
        "#tpr = dict()\n",
        "#roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    print(y_score[:, i])\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "#fpr, tpr, _ = roc_curve(, y_score[:, i])\n",
        "#roc_auc = auc(fpr, tpr)\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "#fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
        "#roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.76301132 -0.36482547  0.12386354]\n",
            " [-0.20224493 -0.63144366 -0.16612302]\n",
            " [ 0.11801481 -0.80263073 -0.32055874]\n",
            " [-0.90780855 -0.12395478  0.02199789]\n",
            " [-0.01116192 -0.27913475 -0.71889214]\n",
            " [-0.6048727  -0.34730509 -0.05859016]\n",
            " [ 0.02283491 -0.24506467 -0.79111998]\n",
            " [-0.61076876  0.18264917 -0.57199363]\n",
            " [-0.37572754 -0.24059516 -0.38933694]\n",
            " [-0.47017411 -0.25745136 -0.27510839]\n",
            " [-0.42224234 -0.30270719 -0.27995197]\n",
            " [-0.3355867  -0.7030665   0.02530178]\n",
            " [-0.22723929 -0.64062258 -0.13456902]\n",
            " [-0.07856729 -0.46354017 -0.45918364]\n",
            " [-0.53383361 -0.2653183  -0.20023832]\n",
            " [ 0.12163662 -0.56706353 -0.56980985]\n",
            " [-0.71356947 -0.04226738 -0.24297128]\n",
            " [-0.55111511 -0.13784913 -0.31370595]\n",
            " [ 0.37991331 -0.99673302 -0.39090964]\n",
            " [-0.11107635 -0.91349462  0.03129167]\n",
            " [-0.70713712 -0.06436533 -0.21423788]\n",
            " [-0.02392675 -0.45906496 -0.51922684]\n",
            " [-0.25045747 -0.80086123  0.04121338]\n",
            " [ 0.12675547 -0.70985659 -0.41072849]\n",
            " [-0.68210402 -0.20735021 -0.12051204]\n",
            " [-0.08001795 -0.36698232 -0.57704892]\n",
            " [-0.03259341 -0.1159895  -0.86493066]\n",
            " [-0.04953425 -0.73611276 -0.21682409]\n",
            " [-0.12974835 -0.37676258 -0.49997476]\n",
            " [-0.19299299 -0.71078341 -0.11058011]\n",
            " [-0.3619768  -0.41408367 -0.22759345]\n",
            " [-0.22818639 -0.78971942  0.02046723]\n",
            " [-0.06196433 -0.47617037 -0.45379557]\n",
            " [-0.52455061 -0.46507392 -0.00375631]\n",
            " [-0.40026409 -0.71470221  0.10106561]\n",
            " [-0.35056585 -0.31125083 -0.34020065]\n",
            " [-0.05770139 -0.51388968 -0.41776502]\n",
            " [-1.11907501 -0.0074193   0.12967625]\n",
            " [ 0.19599366 -0.65773489 -0.54610377]\n",
            " [-0.04299172 -0.60049718 -0.35901924]\n",
            " [-0.48108269 -0.21918849 -0.30065047]\n",
            " [ 0.1741885  -1.0107504  -0.181261  ]\n",
            " [-0.41416456 -0.60044961  0.00856393]\n",
            " [-0.01053513 -0.7579771  -0.2292247 ]\n",
            " [ 0.01645355 -0.81552421 -0.2039252 ]\n",
            " [-0.11932181 -0.84787471 -0.05831557]\n",
            " [-0.70817199 -0.2863326  -0.01186087]\n",
            " [-0.77303401 -0.43228203  0.21326435]\n",
            " [-0.61489613 -0.15060119 -0.23302033]\n",
            " [-0.96334774 -0.62804881  0.58423201]\n",
            " [-0.31037723 -0.29572764 -0.39404258]\n",
            " [-0.31952657 -0.34638653 -0.32086131]\n",
            " [-0.35306417 -0.66917752  0.00767521]\n",
            " [ 0.12127427 -0.62483455 -0.50550427]\n",
            " [-0.6643231  -0.11456775 -0.21298739]\n",
            " [-0.55149778 -0.34855346 -0.10551977]\n",
            " [-0.55695146 -0.13384038 -0.30613086]\n",
            " [-0.41111447 -0.52487765 -0.07455313]\n",
            " [-0.49463336 -0.23331763 -0.27802284]\n",
            " [ 0.06910059 -0.85448531 -0.21662877]\n",
            " [-0.23036784 -0.48759987 -0.28317657]\n",
            " [ 0.30342285 -0.83392076 -0.47754831]\n",
            " [ 0.17642852 -0.81597935 -0.3755452 ]\n",
            " [-0.1906155  -0.70826295 -0.10238744]\n",
            " [-0.42910413 -0.39894364 -0.1693745 ]\n",
            " [-0.67759563  0.09194626 -0.3995789 ]\n",
            " [-0.32958811 -0.56572577 -0.12075396]\n",
            " [-0.97119543 -0.46484965  0.41477557]\n",
            " [ 0.02088168 -0.56912947 -0.44616888]\n",
            " [-0.08177305 -0.5611945  -0.35229343]\n",
            " [-0.41466962 -0.63705856  0.04838688]\n",
            " [-0.30436228 -0.08425378 -0.61864694]\n",
            " [ 0.18869727 -0.8879586  -0.29713077]\n",
            " [ 0.24966175 -0.80507517 -0.44324457]\n",
            " [-0.39980476 -0.29016769 -0.30413406]] [[0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 1 0]]\n",
            "[-0.76301132 -0.20224493  0.11801481 -0.90780855 -0.01116192 -0.6048727\n",
            "  0.02283491 -0.61076876 -0.37572754 -0.47017411 -0.42224234 -0.3355867\n",
            " -0.22723929 -0.07856729 -0.53383361  0.12163662 -0.71356947 -0.55111511\n",
            "  0.37991331 -0.11107635 -0.70713712 -0.02392675 -0.25045747  0.12675547\n",
            " -0.68210402 -0.08001795 -0.03259341 -0.04953425 -0.12974835 -0.19299299\n",
            " -0.3619768  -0.22818639 -0.06196433 -0.52455061 -0.40026409 -0.35056585\n",
            " -0.05770139 -1.11907501  0.19599366 -0.04299172 -0.48108269  0.1741885\n",
            " -0.41416456 -0.01053513  0.01645355 -0.11932181 -0.70817199 -0.77303401\n",
            " -0.61489613 -0.96334774 -0.31037723 -0.31952657 -0.35306417  0.12127427\n",
            " -0.6643231  -0.55149778 -0.55695146 -0.41111447 -0.49463336  0.06910059\n",
            " -0.23036784  0.30342285  0.17642852 -0.1906155  -0.42910413 -0.67759563\n",
            " -0.32958811 -0.97119543  0.02088168 -0.08177305 -0.41466962 -0.30436228\n",
            "  0.18869727  0.24966175 -0.39980476]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-140-7000e0e9ae57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ]
    }
  ]
}